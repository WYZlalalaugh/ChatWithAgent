"""
æ€§èƒ½æµ‹è¯•
"""

import pytest
import time
import asyncio
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from fastapi.testclient import TestClient
import statistics
from unittest.mock import patch, MagicMock

from tests.conftest import assert_response_ok


@pytest.mark.performance
class TestAPIPerformance:
    """APIæ€§èƒ½æµ‹è¯•ç±»"""
    
    def test_auth_endpoint_performance(self, client: TestClient):
        """æµ‹è¯•è®¤è¯ç«¯ç‚¹æ€§èƒ½"""
        # æ³¨å†Œç”¨æˆ·
        user_data = {
            "username": "perf_user",
            "email": "perf@test.com",
            "password": "perfpass123",
            "role": "user"
        }
        
        register_response = client.post("/api/v1/auth/register", json=user_data)
        if register_response.status_code != 201:
            pytest.skip("ç”¨æˆ·æ³¨å†Œå¤±è´¥ï¼Œè·³è¿‡æ€§èƒ½æµ‹è¯•")
        
        register_data = register_response.json()
        token = register_data["access_token"]
        headers = {"Authorization": f"Bearer {token}"}
        
        # æµ‹è¯•è®¤è¯ç«¯ç‚¹æ€§èƒ½
        response_times = []
        
        for i in range(50):
            start_time = time.time()
            response = client.get("/api/v1/auth/me", headers=headers)
            end_time = time.time()
            
            if response.status_code == 200:
                response_times.append(end_time - start_time)
        
        if response_times:
            avg_time = statistics.mean(response_times)
            median_time = statistics.median(response_times)
            max_time = max(response_times)
            min_time = min(response_times)
            
            print(f"\nğŸ“Š è®¤è¯ç«¯ç‚¹æ€§èƒ½ç»Ÿè®¡:")
            print(f"   å¹³å‡å“åº”æ—¶é—´: {avg_time:.3f}ç§’")
            print(f"   ä¸­ä½æ•°å“åº”æ—¶é—´: {median_time:.3f}ç§’")
            print(f"   æœ€å¤§å“åº”æ—¶é—´: {max_time:.3f}ç§’")
            print(f"   æœ€å°å“åº”æ—¶é—´: {min_time:.3f}ç§’")
            
            # æ€§èƒ½æ–­è¨€
            assert avg_time < 1.0, f"å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {avg_time:.3f}ç§’"
            assert max_time < 5.0, f"æœ€å¤§å“åº”æ—¶é—´è¿‡é•¿: {max_time:.3f}ç§’"
    
    def test_bot_crud_performance(self, client: TestClient, auth_headers: dict):
        """æµ‹è¯•æœºå™¨äººCRUDæ“ä½œæ€§èƒ½"""
        bot_data = {
            "name": "æ€§èƒ½æµ‹è¯•æœºå™¨äºº",
            "platform_type": "web",
            "platform_config": {"test": "config"},
            "llm_config": {"provider": "openai", "model": "gpt-3.5-turbo", "api_key": "test"}
        }
        
        # æµ‹è¯•åˆ›å»ºæ€§èƒ½
        create_times = []
        bot_ids = []
        
        for i in range(20):
            bot_data["name"] = f"æ€§èƒ½æµ‹è¯•æœºå™¨äºº{i+1}"
            
            start_time = time.time()
            response = client.post("/api/v1/bots/", json=bot_data, headers=auth_headers)
            end_time = time.time()
            
            if response.status_code == 201:
                create_times.append(end_time - start_time)
                bot_info = response.json()
                bot_ids.append(bot_info["id"])
        
        # æµ‹è¯•è¯»å–æ€§èƒ½
        read_times = []
        
        for bot_id in bot_ids[:10]:  # æµ‹è¯•å‰10ä¸ª
            start_time = time.time()
            response = client.get(f"/api/v1/bots/{bot_id}", headers=auth_headers)
            end_time = time.time()
            
            if response.status_code == 200:
                read_times.append(end_time - start_time)
        
        # æµ‹è¯•åˆ—è¡¨æŸ¥è¯¢æ€§èƒ½
        list_times = []
        
        for i in range(10):
            start_time = time.time()
            response = client.get("/api/v1/bots/", headers=auth_headers)
            end_time = time.time()
            
            if response.status_code == 200:
                list_times.append(end_time - start_time)
        
        # æµ‹è¯•æ›´æ–°æ€§èƒ½
        update_times = []
        
        for bot_id in bot_ids[:5]:  # æµ‹è¯•å‰5ä¸ª
            update_data = {"description": f"æ›´æ–°æè¿° {time.time()}"}
            
            start_time = time.time()
            response = client.put(f"/api/v1/bots/{bot_id}", json=update_data, headers=auth_headers)
            end_time = time.time()
            
            if response.status_code == 200:
                update_times.append(end_time - start_time)
        
        # æµ‹è¯•åˆ é™¤æ€§èƒ½
        delete_times = []
        
        for bot_id in bot_ids:
            start_time = time.time()
            response = client.delete(f"/api/v1/bots/{bot_id}", headers=auth_headers)
            end_time = time.time()
            
            if response.status_code in [200, 204]:
                delete_times.append(end_time - start_time)
        
        # è¾“å‡ºæ€§èƒ½ç»Ÿè®¡
        operations = {
            "åˆ›å»º": create_times,
            "è¯»å–": read_times,
            "åˆ—è¡¨": list_times,
            "æ›´æ–°": update_times,
            "åˆ é™¤": delete_times
        }
        
        print(f"\nğŸ“Š æœºå™¨äººCRUDæ€§èƒ½ç»Ÿè®¡:")
        
        for op_name, times in operations.items():
            if times:
                avg_time = statistics.mean(times)
                print(f"   {op_name}æ“ä½œå¹³å‡æ—¶é—´: {avg_time:.3f}ç§’")
                
                # æ€§èƒ½æ–­è¨€
                assert avg_time < 2.0, f"{op_name}æ“ä½œå¹³å‡æ—¶é—´è¿‡é•¿: {avg_time:.3f}ç§’"
    
    def test_concurrent_requests_performance(self, client: TestClient, auth_headers: dict):
        """æµ‹è¯•å¹¶å‘è¯·æ±‚æ€§èƒ½"""
        def make_request():
            start_time = time.time()
            response = client.get("/api/v1/auth/me", headers=auth_headers)
            end_time = time.time()
            
            return {
                "status_code": response.status_code,
                "response_time": end_time - start_time,
                "success": response.status_code == 200
            }
        
        # å¹¶å‘æµ‹è¯•
        concurrent_users = 20
        requests_per_user = 5
        
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = []
            
            for _ in range(concurrent_users * requests_per_user):
                future = executor.submit(make_request)
                futures.append(future)
            
            results = []
            for future in as_completed(futures):
                result = future.result()
                results.append(result)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # ç»Ÿè®¡ç»“æœ
        successful_requests = sum(1 for r in results if r["success"])
        failed_requests = len(results) - successful_requests
        response_times = [r["response_time"] for r in results if r["success"]]
        
        if response_times:
            avg_response_time = statistics.mean(response_times)
            throughput = successful_requests / total_time
            
            print(f"\nğŸ“Š å¹¶å‘è¯·æ±‚æ€§èƒ½ç»Ÿè®¡:")
            print(f"   å¹¶å‘ç”¨æˆ·æ•°: {concurrent_users}")
            print(f"   æ€»è¯·æ±‚æ•°: {len(results)}")
            print(f"   æˆåŠŸè¯·æ±‚æ•°: {successful_requests}")
            print(f"   å¤±è´¥è¯·æ±‚æ•°: {failed_requests}")
            print(f"   æ€»è€—æ—¶: {total_time:.3f}ç§’")
            print(f"   å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}ç§’")
            print(f"   ååé‡: {throughput:.2f} è¯·æ±‚/ç§’")
            
            # æ€§èƒ½æ–­è¨€
            success_rate = successful_requests / len(results)
            assert success_rate >= 0.95, f"æˆåŠŸç‡è¿‡ä½: {success_rate*100:.1f}%"
            assert avg_response_time < 3.0, f"å¹¶å‘å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {avg_response_time:.3f}ç§’"
            assert throughput >= 5.0, f"ååé‡è¿‡ä½: {throughput:.2f} è¯·æ±‚/ç§’"
    
    def test_large_data_query_performance(self, client: TestClient, auth_headers: dict):
        """æµ‹è¯•å¤§æ•°æ®é‡æŸ¥è¯¢æ€§èƒ½"""
        # åˆ›å»ºå¤§é‡æµ‹è¯•æ•°æ®
        bot_data = {
            "name": "å¤§æ•°æ®æµ‹è¯•æœºå™¨äºº",
            "platform_type": "web", 
            "platform_config": {"test": "config"},
            "llm_config": {"provider": "openai", "model": "gpt-3.5-turbo", "api_key": "test"}
        }
        
        created_bots = []
        
        # åˆ›å»º100ä¸ªæœºå™¨äººï¼ˆå¦‚æœç³»ç»Ÿèƒ½å¤„ç†ï¼‰
        for i in range(min(100, 50)):  # é™åˆ¶åœ¨50ä¸ªä»¥å†…ï¼Œé¿å…æµ‹è¯•æ—¶é—´è¿‡é•¿
            bot_data["name"] = f"å¤§æ•°æ®æµ‹è¯•æœºå™¨äºº{i+1}"
            
            response = client.post("/api/v1/bots/", json=bot_data, headers=auth_headers)
            
            if response.status_code == 201:
                bot_info = response.json()
                created_bots.append(bot_info["id"])
            else:
                break
        
        if not created_bots:
            pytest.skip("æ— æ³•åˆ›å»ºæµ‹è¯•æ•°æ®ï¼Œè·³è¿‡å¤§æ•°æ®æŸ¥è¯¢æµ‹è¯•")
        
        # æµ‹è¯•ä¸åŒåˆ†é¡µå¤§å°çš„æŸ¥è¯¢æ€§èƒ½
        page_sizes = [10, 20, 50, 100]
        
        print(f"\nğŸ“Š å¤§æ•°æ®æŸ¥è¯¢æ€§èƒ½æµ‹è¯• (æ€»æ•°æ®é‡: {len(created_bots)}):")
        
        try:
            for page_size in page_sizes:
                query_times = []
                
                for page in range(3):  # æµ‹è¯•å‰3é¡µ
                    start_time = time.time()
                    response = client.get(
                        f"/api/v1/bots/?limit={page_size}&offset={page * page_size}",
                        headers=auth_headers
                    )
                    end_time = time.time()
                    
                    if response.status_code == 200:
                        query_times.append(end_time - start_time)
                
                if query_times:
                    avg_time = statistics.mean(query_times)
                    print(f"   åˆ†é¡µå¤§å° {page_size}: å¹³å‡æŸ¥è¯¢æ—¶é—´ {avg_time:.3f}ç§’")
                    
                    # æ€§èƒ½æ–­è¨€
                    assert avg_time < 5.0, f"åˆ†é¡µå¤§å°{page_size}æŸ¥è¯¢æ—¶é—´è¿‡é•¿: {avg_time:.3f}ç§’"
        
        finally:
            # æ¸…ç†æµ‹è¯•æ•°æ®
            for bot_id in created_bots:
                client.delete(f"/api/v1/bots/{bot_id}", headers=auth_headers)
    
    @patch('multimodal.process_binary')
    def test_file_processing_performance(self, mock_process, client: TestClient, auth_headers: dict):
        """æµ‹è¯•æ–‡ä»¶å¤„ç†æ€§èƒ½"""
        # æ¨¡æ‹Ÿæ–‡ä»¶å¤„ç†ç»“æœ
        mock_result = MagicMock()
        mock_result.success = True
        mock_result.media_type.value = "text"
        mock_result.content = "å¤„ç†åçš„å†…å®¹"
        mock_result.metadata = {"size": 1024}
        mock_result.error = None
        mock_result.processed_files = ["test.txt"]
        mock_process.return_value = mock_result
        
        # æµ‹è¯•ä¸åŒå¤§å°æ–‡ä»¶çš„å¤„ç†æ€§èƒ½
        file_sizes = [1024, 10240, 102400]  # 1KB, 10KB, 100KB
        
        print(f"\nğŸ“Š æ–‡ä»¶å¤„ç†æ€§èƒ½æµ‹è¯•:")
        
        for size in file_sizes:
            file_content = b"x" * size
            
            processing_times = []
            
            for i in range(5):
                files = {"file": (f"test_{size}.txt", file_content, "text/plain")}
                
                start_time = time.time()
                response = client.post(
                    "/api/v1/multimodal/process",
                    files=files,
                    data={"options": "{}"},
                    headers=auth_headers
                )
                end_time = time.time()
                
                if response.status_code == 200:
                    processing_times.append(end_time - start_time)
                elif response.status_code == 404:
                    # å¤šæ¨¡æ€APIæœªå®ç°
                    break
            
            if processing_times:
                avg_time = statistics.mean(processing_times)
                throughput = size / avg_time / 1024  # KB/s
                
                print(f"   æ–‡ä»¶å¤§å° {size/1024:.1f}KB: å¹³å‡å¤„ç†æ—¶é—´ {avg_time:.3f}ç§’, ååé‡ {throughput:.1f}KB/s")
                
                # æ€§èƒ½æ–­è¨€
                assert avg_time < 10.0, f"æ–‡ä»¶å¤„ç†æ—¶é—´è¿‡é•¿: {avg_time:.3f}ç§’"


@pytest.mark.performance
@pytest.mark.asyncio
class TestAsyncPerformance:
    """å¼‚æ­¥æ€§èƒ½æµ‹è¯•ç±»"""
    
    async def test_concurrent_database_operations(self):
        """æµ‹è¯•å¹¶å‘æ•°æ®åº“æ“ä½œæ€§èƒ½"""
        try:
            from managers.bot_manager import bot_manager
            
            # å¹¶å‘åˆ›å»ºæœºå™¨äºº
            async def create_bot_task(index):
                bot_data = {
                    "name": f"å¹¶å‘æµ‹è¯•æœºå™¨äºº{index}",
                    "platform_type": "web",
                    "platform_config": {"test": f"config{index}"},
                    "llm_config": {"provider": "openai", "model": "gpt-3.5-turbo", "api_key": "test"}
                }
                
                start_time = time.time()
                try:
                    # è¿™é‡Œéœ€è¦mockç”¨æˆ·ID
                    result = await bot_manager.create_bot(
                        user_id="test_user_id",
                        **bot_data
                    )
                    end_time = time.time()
                    
                    return {
                        "success": True,
                        "time": end_time - start_time,
                        "bot_id": result.id if result else None
                    }
                except Exception as e:
                    end_time = time.time()
                    return {
                        "success": False,
                        "time": end_time - start_time,
                        "error": str(e)
                    }
            
            # æ‰§è¡Œå¹¶å‘ä»»åŠ¡
            concurrent_tasks = 10
            start_time = time.time()
            
            tasks = [create_bot_task(i) for i in range(concurrent_tasks)]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            end_time = time.time()
            total_time = end_time - start_time
            
            # ç»Ÿè®¡ç»“æœ
            successful_ops = sum(1 for r in results if isinstance(r, dict) and r.get("success"))
            operation_times = [r["time"] for r in results if isinstance(r, dict) and "time" in r]
            
            if operation_times:
                avg_time = statistics.mean(operation_times)
                
                print(f"\nğŸ“Š å¹¶å‘æ•°æ®åº“æ“ä½œæ€§èƒ½:")
                print(f"   å¹¶å‘ä»»åŠ¡æ•°: {concurrent_tasks}")
                print(f"   æˆåŠŸæ“ä½œæ•°: {successful_ops}")
                print(f"   æ€»è€—æ—¶: {total_time:.3f}ç§’")
                print(f"   å¹³å‡æ“ä½œæ—¶é—´: {avg_time:.3f}ç§’")
                
                # æ€§èƒ½æ–­è¨€
                assert avg_time < 5.0, f"å¹¶å‘æ•°æ®åº“æ“ä½œæ—¶é—´è¿‡é•¿: {avg_time:.3f}ç§’"
        
        except ImportError:
            pytest.skip("Manager modules not available")
    
    async def test_streaming_performance(self):
        """æµ‹è¯•æµå¼å¤„ç†æ€§èƒ½"""
        try:
            from engines.conversation_engine import conversation_engine
            
            # æ¨¡æ‹Ÿæµå¼å¤„ç†
            start_time = time.time()
            
            chunks_received = 0
            total_content_length = 0
            
            async for chunk in conversation_engine.process_message(
                conversation_id="test_conv_id",
                user_message="è¯·ç”Ÿæˆä¸€ä¸ªé•¿å›ç­”",
                bot_config={
                    "llm_config": {"provider": "openai", "model": "gpt-3.5-turbo"},
                    "system_prompt": "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹",
                    "knowledge_base_ids": [],
                    "plugins": []
                },
                stream=True
            ):
                chunks_received += 1
                if chunk.get("type") == "content":
                    content = chunk.get("content", "")
                    total_content_length += len(content)
                elif chunk.get("type") == "response_complete":
                    break
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            if chunks_received > 0:
                avg_chunk_time = processing_time / chunks_received
                
                print(f"\nğŸ“Š æµå¼å¤„ç†æ€§èƒ½:")
                print(f"   æ€»å¤„ç†æ—¶é—´: {processing_time:.3f}ç§’")
                print(f"   æ¥æ”¶å—æ•°: {chunks_received}")
                print(f"   æ€»å†…å®¹é•¿åº¦: {total_content_length}")
                print(f"   å¹³å‡å—å¤„ç†æ—¶é—´: {avg_chunk_time:.3f}ç§’")
                
                # æ€§èƒ½æ–­è¨€
                assert avg_chunk_time < 1.0, f"æµå¼å¤„ç†å—æ—¶é—´è¿‡é•¿: {avg_chunk_time:.3f}ç§’"
        
        except ImportError:
            pytest.skip("Conversation engine not available")


@pytest.mark.performance
class TestMemoryPerformance:
    """å†…å­˜æ€§èƒ½æµ‹è¯•ç±»"""
    
    def test_memory_usage_monitoring(self, client: TestClient, auth_headers: dict):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µç›‘æ§"""
        import psutil
        import os
        
        # è·å–å½“å‰è¿›ç¨‹
        process = psutil.Process(os.getpid())
        
        # è®°å½•åˆå§‹å†…å­˜ä½¿ç”¨
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # æ‰§è¡Œä¸€ç³»åˆ—æ“ä½œ
        for i in range(10):
            # åˆ›å»ºæœºå™¨äºº
            bot_data = {
                "name": f"å†…å­˜æµ‹è¯•æœºå™¨äºº{i}",
                "platform_type": "web",
                "platform_config": {"test": f"config{i}"},
                "llm_config": {"provider": "openai", "model": "gpt-3.5-turbo", "api_key": "test"}
            }
            
            response = client.post("/api/v1/bots/", json=bot_data, headers=auth_headers)
            
            if response.status_code == 201:
                bot_info = response.json()
                
                # ç«‹å³åˆ é™¤ä»¥æµ‹è¯•å†…å­˜é‡Šæ”¾
                client.delete(f"/api/v1/bots/{bot_info['id']}", headers=auth_headers)
        
        # è®°å½•æœ€ç»ˆå†…å­˜ä½¿ç”¨
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_growth = final_memory - initial_memory
        
        print(f"\nğŸ“Š å†…å­˜ä½¿ç”¨æ€§èƒ½:")
        print(f"   åˆå§‹å†…å­˜: {initial_memory:.2f}MB")
        print(f"   æœ€ç»ˆå†…å­˜: {final_memory:.2f}MB")
        print(f"   å†…å­˜å¢é•¿: {memory_growth:.2f}MB")
        
        # å†…å­˜å¢é•¿åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
        assert memory_growth < 100, f"å†…å­˜å¢é•¿è¿‡å¤š: {memory_growth:.2f}MB"
    
    def test_garbage_collection_performance(self, client: TestClient, auth_headers: dict):
        """æµ‹è¯•åƒåœ¾å›æ”¶æ€§èƒ½"""
        import gc
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # è®°å½•GCç»Ÿè®¡
        initial_collections = gc.get_stats()
        
        # æ‰§è¡Œä¸€äº›æ“ä½œäº§ç”Ÿåƒåœ¾å¯¹è±¡
        large_data_list = []
        
        for i in range(100):
            response = client.get("/api/v1/auth/me", headers=auth_headers)
            
            if response.status_code == 200:
                # ä¿å­˜å“åº”æ•°æ®ï¼ˆæ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨ï¼‰
                large_data_list.append(response.json())
        
        # æ¸…ç†å¼•ç”¨
        del large_data_list
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        collected = gc.collect()
        
        # è®°å½•æœ€ç»ˆGCç»Ÿè®¡
        final_collections = gc.get_stats()
        
        print(f"\nğŸ“Š åƒåœ¾å›æ”¶æ€§èƒ½:")
        print(f"   æ‰‹åŠ¨å›æ”¶å¯¹è±¡æ•°: {collected}")
        
        # åƒåœ¾å›æ”¶åº”è¯¥æœ‰æ•ˆ
        assert collected >= 0, "åƒåœ¾å›æ”¶å¤±è´¥"


@pytest.mark.performance
class TestDatabasePerformance:
    """æ•°æ®åº“æ€§èƒ½æµ‹è¯•ç±»"""
    
    def test_database_connection_performance(self, client: TestClient, auth_headers: dict):
        """æµ‹è¯•æ•°æ®åº“è¿æ¥æ€§èƒ½"""
        # æµ‹è¯•æ•°æ®åº“è¿æ¥çš„å»ºç«‹å’Œå…³é—­æ€§èƒ½
        
        connection_times = []
        
        for i in range(10):
            start_time = time.time()
            
            # æ‰§è¡Œæ•°æ®åº“æ“ä½œï¼ˆé€šè¿‡APIé—´æ¥æµ‹è¯•ï¼‰
            response = client.get("/api/v1/bots/", headers=auth_headers)
            
            end_time = time.time()
            
            if response.status_code == 200:
                connection_times.append(end_time - start_time)
        
        if connection_times:
            avg_time = statistics.mean(connection_times)
            
            print(f"\nğŸ“Š æ•°æ®åº“è¿æ¥æ€§èƒ½:")
            print(f"   å¹³å‡è¿æ¥æ—¶é—´: {avg_time:.3f}ç§’")
            
            # æ•°æ®åº“è¿æ¥åº”è¯¥å¾ˆå¿«
            assert avg_time < 2.0, f"æ•°æ®åº“è¿æ¥æ—¶é—´è¿‡é•¿: {avg_time:.3f}ç§’"
    
    def test_transaction_performance(self, client: TestClient, auth_headers: dict):
        """æµ‹è¯•æ•°æ®åº“äº‹åŠ¡æ€§èƒ½"""
        # æµ‹è¯•å¤æ‚äº‹åŠ¡çš„æ€§èƒ½
        
        transaction_times = []
        created_bots = []
        
        try:
            for i in range(5):
                start_time = time.time()
                
                # åˆ›å»ºæœºå™¨äººï¼ˆæ¶‰åŠæ•°æ®åº“äº‹åŠ¡ï¼‰
                bot_data = {
                    "name": f"äº‹åŠ¡æµ‹è¯•æœºå™¨äºº{i}",
                    "platform_type": "web",
                    "platform_config": {"test": f"config{i}"},
                    "llm_config": {"provider": "openai", "model": "gpt-3.5-turbo", "api_key": "test"}
                }
                
                response = client.post("/api/v1/bots/", json=bot_data, headers=auth_headers)
                
                end_time = time.time()
                
                if response.status_code == 201:
                    transaction_times.append(end_time - start_time)
                    bot_info = response.json()
                    created_bots.append(bot_info["id"])
            
            if transaction_times:
                avg_time = statistics.mean(transaction_times)
                
                print(f"\nğŸ“Š æ•°æ®åº“äº‹åŠ¡æ€§èƒ½:")
                print(f"   å¹³å‡äº‹åŠ¡æ—¶é—´: {avg_time:.3f}ç§’")
                
                # äº‹åŠ¡æ—¶é—´åº”è¯¥åˆç†
                assert avg_time < 3.0, f"æ•°æ®åº“äº‹åŠ¡æ—¶é—´è¿‡é•¿: {avg_time:.3f}ç§’"
        
        finally:
            # æ¸…ç†æµ‹è¯•æ•°æ®
            for bot_id in created_bots:
                client.delete(f"/api/v1/bots/{bot_id}", headers=auth_headers)


def pytest_configure(config):
    """pytesté…ç½®"""
    config.addinivalue_line(
        "markers", 
        "performance: marks tests as performance tests (deselect with '-m \"not performance\"')"
    )